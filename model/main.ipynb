{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc43f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import models.gaussian_diffusion as gd\n",
    "from models.DNN import GDN\n",
    "import evaluate_utils\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d522bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)  # cpu\n",
    "torch.cuda.manual_seed(random_seed)  # gpu\n",
    "np.random.seed(random_seed)  # numpy\n",
    "random.seed(random_seed)  # random and transforms\n",
    "torch.backends.cudnn.deterministic=True  # cudnn\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(random_seed + worker_id)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b2d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(batch_size=400, cuda=False, data_path='../datasets/', dataset='ml-1m', drop_out=0.1, emb_size=10, epochs=300, gpu='0', graph_layers=1, graph_views=1, log_name='log', lr=0.0001, mean_type='x0', mlp_hidden_dims='[1000]', noise_max=0.005, noise_min=0.0005, noise_scale=1.0, noise_schedule='linear-var', norm=True, reweight=True, round=1, sample_style='uniform', sampling_noise=False, sampling_steps=0, save_path='./saved_models/', steps=2, time_type='add', topN='[10, 20, 50, 100]', tst_w_val=False, w_max=1.0, w_min=0.1, weight_decay=0.0)\n",
      "Starting time:  2023-11-11 17:11:53\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='ml-1m', help='choose the dataset')\n",
    "parser.add_argument('--data_path', type=str, default='../datasets/', help='load data path')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='learning rate')\n",
    "parser.add_argument('--drop_out', type=float, default=0.1, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "parser.add_argument('--batch_size', type=int, default=400)\n",
    "parser.add_argument('--epochs', type=int, default=300, help='upper epoch limit')\n",
    "parser.add_argument('--topN', type=str, default='[10, 20, 50, 100]')\n",
    "parser.add_argument('--tst_w_val', action='store_true', help='test with validation')\n",
    "parser.add_argument('--cuda', action='store_true', help='use CUDA')\n",
    "parser.add_argument('--gpu', type=str, default='0', help='gpu card ID')\n",
    "parser.add_argument('--save_path', type=str, default='./saved_models/', help='save model path')\n",
    "parser.add_argument('--log_name', type=str, default='log', help='the log name')\n",
    "parser.add_argument('--round', type=int, default=1, help='record the experiment')\n",
    "\n",
    "parser.add_argument('--w_min', type=float, default=0.1, help='the minimum weight for interactions')\n",
    "parser.add_argument('--w_max', type=float, default=1., help='the maximum weight for interactions')\n",
    "\n",
    "# params for the model\n",
    "parser.add_argument('--time_type', type=str, default='add', help='cat or add')\n",
    "parser.add_argument('--graph_layers', type=int, default=1, help='the nums layer for the GNN')\n",
    "parser.add_argument('--graph_views', type=int, default=1, help='the nums views for the GNN')\n",
    "parser.add_argument('--mlp_hidden_dims', type=str, default='[1000]', help='the dims for the DNN')\n",
    "parser.add_argument('--norm', type=bool, default=True, help='Normalize the input or not')\n",
    "parser.add_argument('--emb_size', type=int, default=10, help='timestep embedding size')\n",
    "\n",
    "# params for diffusion\n",
    "parser.add_argument('--sample_style', type=str, default='uniform', help='importance/uniform/fully')\n",
    "parser.add_argument('--mean_type', type=str, default='x0', help='MeanType for diffusion: x0, eps')\n",
    "parser.add_argument('--steps', type=int, default=2, help='diffusion steps')\n",
    "parser.add_argument('--noise_schedule', type=str, default='linear-var', help='the schedule for noise generating')\n",
    "parser.add_argument('--noise_scale', type=float, default=1.0, help='noise scale for noise generating')\n",
    "parser.add_argument('--noise_min', type=float, default=0.0005, help='noise lower bound for noise generating')\n",
    "parser.add_argument('--noise_max', type=float, default=0.005, help='noise upper bound for noise generating')\n",
    "parser.add_argument('--sampling_noise', type=bool, default=False, help='sampling with noise or not')\n",
    "parser.add_argument('--sampling_steps', type=int, default=0, help='steps of the forward process during inference')\n",
    "parser.add_argument('--reweight', type=bool, default=True, help='assign different weight to different timestep or not')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "print(\"args:\", args)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(\"Starting time: \", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0917ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user num: 5949\n",
      "item num: 2810\n",
      "data ready.\n",
      "models ready.\n",
      "Number of all parameters: 5633920\n"
     ]
    }
   ],
   "source": [
    "### DATA LOAD ###\n",
    "train_path = os.path.join(args.data_path, args.dataset, 'train_list.npy')\n",
    "valid_path = os.path.join(args.data_path, args.dataset, 'valid_list.npy')\n",
    "test_path = os.path.join(args.data_path, args.dataset, 'test_list.npy')\n",
    "\n",
    "train_data, train_data_ori, valid_y_data, test_y_data, n_user, n_item, g = data_utils.data_load(train_path, valid_path, test_path, args.w_min, args.w_max)\n",
    "train_dataset = data_utils.DataDiffusion(torch.FloatTensor(train_data.A))\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, shuffle=True, worker_init_fn=worker_init_fn)\n",
    "test_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "if args.tst_w_val:\n",
    "    tv_dataset = data_utils.DataDiffusion(torch.FloatTensor(train_data.A) + torch.FloatTensor(valid_y_data.A))\n",
    "    test_twv_loader = DataLoader(tv_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "mask_tv = train_data_ori + valid_y_data\n",
    "\n",
    "print('data ready.')\n",
    "\n",
    "\n",
    "### Build Gaussian Diffusion ###\n",
    "if args.mean_type == 'x0':\n",
    "    mean_type = gd.ModelMeanType.START_X\n",
    "elif args.mean_type == 'eps':\n",
    "    mean_type = gd.ModelMeanType.EPSILON\n",
    "else:\n",
    "    raise ValueError(\"Unimplemented mean type %s\" % args.mean_type)\n",
    "\n",
    "diffusion = gd.GaussianDiffusion(mean_type, args.noise_schedule, args.noise_scale,\n",
    "                                 args.noise_min, args.noise_max, args.steps, device).to(device)\n",
    "\n",
    "### Build MLP ###\n",
    "if eval(args.mlp_hidden_dims):\n",
    "    mlp_dims = [n_item] + eval(args.mlp_hidden_dims) + [n_item]\n",
    "else:\n",
    "    mlp_dims = [n_item, n_item]\n",
    "model = GDN(mlp_dims, args.emb_size, g, args.graph_layers, norm=args.norm, dropout=args.drop_out).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "print(\"models ready.\")\n",
    "\n",
    "param_num = 0\n",
    "mlp_num = sum([param.nelement() for param in model.parameters()])\n",
    "diff_num = sum([param.nelement() for param in diffusion.parameters()])  # 0\n",
    "param_num = mlp_num + diff_num\n",
    "print(\"Number of all parameters:\", param_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f9978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, data_te, mask_his, topN):\n",
    "    model.eval()\n",
    "    e_idxlist = list(range(mask_his.shape[0]))\n",
    "    e_N = mask_his.shape[0]\n",
    "\n",
    "    predict_items = []\n",
    "    target_items = []\n",
    "    for i in range(e_N):\n",
    "        target_items.append(data_te[i, :].nonzero()[1].tolist())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            his_data = mask_his[e_idxlist[batch_idx*args.batch_size:batch_idx*args.batch_size+len(batch)]]\n",
    "            batch = batch.to(device)\n",
    "            prediction = diffusion.p_sample(model, batch, args.sampling_steps, args.sampling_noise)\n",
    "            prediction[his_data.nonzero()] = -np.inf\n",
    "\n",
    "            _, indices = torch.topk(prediction, topN[-1])\n",
    "            indices = indices.cpu().numpy().tolist()\n",
    "            predict_items.extend(indices)\n",
    "\n",
    "    test_results = evaluate_utils.computeTopNAccuracy(target_items, predict_items, topN)\n",
    "\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33069e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Runing Epoch 001 train loss 5467.7392 costs 00: 00: 06\n",
      "------------------------------------------------------\n",
      "Runing Epoch 002 train loss 5069.3424 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 003 train loss 4978.5873 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 004 train loss 4912.9554 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0641-0.0589-0.0485-0.0404 Recall: 0.0459-0.0826-0.163-0.2596 NDCG: 0.074-0.0821-0.1076-0.1423 MRR: 0.1521-0.1625-0.1683-0.17\n",
      "[Test]: Precision: 0.0507-0.0437-0.0345-0.0275 Recall: 0.0678-0.1118-0.2103-0.3244 NDCG: 0.0712-0.0843-0.1174-0.1525 MRR: 0.1313-0.1406-0.1467-0.1487\n",
      "Runing Epoch 005 train loss 4697.3982 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 006 train loss 4645.5617 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 007 train loss 4499.0594 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 008 train loss 4479.8055 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 009 train loss 4479.0745 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.068-0.0617-0.0528-0.0442 Recall: 0.0564-0.0963-0.1908-0.3001 NDCG: 0.0814-0.0914-0.123-0.1621 MRR: 0.1621-0.1727-0.1791-0.1806\n",
      "[Test]: Precision: 0.0592-0.0517-0.0403-0.0315 Recall: 0.0902-0.1484-0.268-0.3936 NDCG: 0.0885-0.1073-0.1474-0.1863 MRR: 0.1547-0.1651-0.1716-0.1732\n",
      "Runing Epoch 010 train loss 4452.2671 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 011 train loss 4377.9445 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 012 train loss 4242.0756 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 013 train loss 4337.0759 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 014 train loss 4314.6230 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0685-0.063-0.0539-0.046 Recall: 0.0597-0.1031-0.2007-0.3196 NDCG: 0.0833-0.0951-0.1281-0.1707 MRR: 0.164-0.1754-0.1818-0.1834\n",
      "[Test]: Precision: 0.0622-0.054-0.0433-0.0339 Recall: 0.0997-0.1606-0.2938-0.4288 NDCG: 0.095-0.1152-0.1604-0.2022 MRR: 0.1622-0.1727-0.1794-0.181\n",
      "Runing Epoch 015 train loss 4220.2271 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 016 train loss 4315.9738 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 017 train loss 4261.8974 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 018 train loss 4091.1191 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 019 train loss 4175.2472 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0698-0.0631-0.0546-0.0471 Recall: 0.0626-0.1069-0.2104-0.334 NDCG: 0.0846-0.0963-0.1315-0.1758 MRR: 0.1646-0.1763-0.1828-0.1844\n",
      "[Test]: Precision: 0.0642-0.0564-0.0452-0.0353 Recall: 0.1052-0.1707-0.3122-0.4523 NDCG: 0.098-0.1202-0.1682-0.2117 MRR: 0.1648-0.176-0.1828-0.1843\n",
      "Runing Epoch 020 train loss 4141.7308 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 021 train loss 4027.3136 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 022 train loss 4089.8773 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 023 train loss 4111.7871 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 024 train loss 4151.0873 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0698-0.0637-0.0548-0.0474 Recall: 0.0639-0.1107-0.217-0.3411 NDCG: 0.0848-0.0978-0.1339-0.1786 MRR: 0.1641-0.1756-0.1823-0.184\n",
      "[Test]: Precision: 0.0653-0.058-0.0464-0.0363 Recall: 0.1097-0.181-0.3241-0.4682 NDCG: 0.1005-0.1248-0.1736-0.2183 MRR: 0.1675-0.179-0.1858-0.1873\n",
      "Runing Epoch 025 train loss 4158.0799 costs 00: 00: 12\n",
      "------------------------------------------------------\n",
      "Runing Epoch 026 train loss 3937.8999 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 027 train loss 3983.6084 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 028 train loss 3791.6370 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 029 train loss 3821.2764 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0694-0.0637-0.055-0.0474 Recall: 0.0657-0.1127-0.222-0.3462 NDCG: 0.0849-0.0984-0.1355-0.1801 MRR: 0.1629-0.1741-0.1811-0.1827\n",
      "[Test]: Precision: 0.0659-0.0586-0.047-0.0366 Recall: 0.114-0.1863-0.3348-0.4784 NDCG: 0.1025-0.1273-0.1778-0.2224 MRR: 0.1685-0.1807-0.1876-0.189\n",
      "Runing Epoch 030 train loss 3861.1157 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 031 train loss 3886.7504 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 032 train loss 3872.3159 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 033 train loss 3844.7932 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 034 train loss 3921.5187 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.069-0.0632-0.0551-0.0474 Recall: 0.0666-0.114-0.2261-0.3504 NDCG: 0.0851-0.0988-0.137-0.1817 MRR: 0.1631-0.1746-0.1817-0.1834\n",
      "[Test]: Precision: 0.0666-0.0591-0.0474-0.037 Recall: 0.1169-0.192-0.3418-0.4892 NDCG: 0.1041-0.1298-0.1809-0.2267 MRR: 0.1705-0.1828-0.1897-0.1911\n",
      "Runing Epoch 035 train loss 3824.4739 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 036 train loss 3813.9335 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 037 train loss 3758.5896 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 038 train loss 3750.1438 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 039 train loss 3804.3645 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0688-0.0634-0.0549-0.0473 Recall: 0.0668-0.116-0.2273-0.3516 NDCG: 0.0855-0.1-0.1378-0.1826 MRR: 0.1644-0.1761-0.1833-0.1849\n",
      "[Test]: Precision: 0.0673-0.0596-0.0475-0.0373 Recall: 0.1193-0.1958-0.3456-0.4946 NDCG: 0.1053-0.1316-0.1827-0.229 MRR: 0.1699-0.1823-0.1891-0.1906\n",
      "Runing Epoch 040 train loss 3646.0916 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 041 train loss 3698.0848 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 042 train loss 3671.1567 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 043 train loss 3690.4478 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 044 train loss 3728.7451 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0683-0.063-0.0546-0.0472 Recall: 0.0671-0.1159-0.2276-0.3536 NDCG: 0.0853-0.0998-0.1377-0.183 MRR: 0.1641-0.176-0.1832-0.1848\n",
      "[Test]: Precision: 0.0678-0.0604-0.0477-0.0373 Recall: 0.1221-0.1999-0.3503-0.4982 NDCG: 0.1069-0.1339-0.1851-0.2311 MRR: 0.1725-0.185-0.1918-0.1932\n",
      "Runing Epoch 045 train loss 3617.0935 costs 00: 00: 12\n",
      "------------------------------------------------------\n",
      "Runing Epoch 046 train loss 3601.4854 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 047 train loss 3693.8834 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 048 train loss 3702.5492 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 049 train loss 3623.5682 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0678-0.0625-0.0547-0.0471 Recall: 0.0679-0.1164-0.2285-0.3527 NDCG: 0.0849-0.0995-0.1379-0.1826 MRR: 0.1625-0.1746-0.1818-0.1834\n",
      "[Test]: Precision: 0.0676-0.0603-0.0478-0.0375 Recall: 0.1234-0.2016-0.3522-0.5004 NDCG: 0.1077-0.1349-0.1863-0.2325 MRR: 0.1741-0.1865-0.1933-0.1947\n",
      "Runing Epoch 050 train loss 3689.2340 costs 00: 00: 11\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing Epoch 051 train loss 3548.0584 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 052 train loss 3495.2280 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 053 train loss 3575.7855 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 054 train loss 3506.2596 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0675-0.0625-0.0544-0.047 Recall: 0.0678-0.1171-0.2287-0.354 NDCG: 0.0844-0.0993-0.1375-0.1826 MRR: 0.1623-0.1743-0.1816-0.1832\n",
      "[Test]: Precision: 0.0686-0.0605-0.0479-0.0375 Recall: 0.1265-0.2039-0.3545-0.5028 NDCG: 0.1092-0.136-0.1875-0.2337 MRR: 0.1759-0.1879-0.1947-0.1961\n",
      "Runing Epoch 055 train loss 3523.7544 costs 00: 00: 12\n",
      "------------------------------------------------------\n",
      "Runing Epoch 056 train loss 3471.8115 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 057 train loss 3432.6178 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 058 train loss 3556.4437 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 059 train loss 3601.6555 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0672-0.0622-0.0542-0.0468 Recall: 0.0677-0.1175-0.228-0.3527 NDCG: 0.0841-0.0992-0.1372-0.182 MRR: 0.1613-0.1736-0.1808-0.1824\n",
      "[Test]: Precision: 0.0685-0.061-0.0479-0.0376 Recall: 0.1265-0.2058-0.3556-0.5024 NDCG: 0.1096-0.1373-0.1884-0.2346 MRR: 0.1778-0.1899-0.1966-0.1981\n",
      "Runing Epoch 060 train loss 3599.5141 costs 00: 00: 12\n",
      "------------------------------------------------------\n",
      "Runing Epoch 061 train loss 3595.2669 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 062 train loss 3564.4504 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 063 train loss 3525.7466 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 064 train loss 3406.9893 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.067-0.0621-0.0543-0.0468 Recall: 0.0671-0.1172-0.2283-0.3525 NDCG: 0.0838-0.0989-0.1371-0.1818 MRR: 0.161-0.1733-0.1805-0.182\n",
      "[Test]: Precision: 0.0685-0.0605-0.0482-0.0376 Recall: 0.1267-0.2042-0.3587-0.5047 NDCG: 0.1104-0.1375-0.1901-0.2358 MRR: 0.1793-0.1915-0.1984-0.1998\n",
      "Runing Epoch 065 train loss 3435.6144 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 066 train loss 3375.2939 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 067 train loss 3515.1595 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 068 train loss 3391.4973 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 069 train loss 3251.5894 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0666-0.0616-0.0542-0.0468 Recall: 0.0668-0.1171-0.2279-0.3515 NDCG: 0.0834-0.0984-0.1368-0.1813 MRR: 0.1603-0.1724-0.1799-0.1814\n",
      "[Test]: Precision: 0.0686-0.0607-0.0482-0.0375 Recall: 0.1279-0.2058-0.3586-0.5015 NDCG: 0.1109-0.1381-0.1902-0.2352 MRR: 0.1805-0.1927-0.1994-0.2008\n",
      "Runing Epoch 070 train loss 3431.8113 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 071 train loss 3406.8138 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 072 train loss 3243.8401 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 073 train loss 3444.7243 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 074 train loss 3363.2716 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0663-0.0615-0.0539-0.0465 Recall: 0.0669-0.1177-0.227-0.3503 NDCG: 0.0828-0.0981-0.136-0.1803 MRR: 0.1585-0.1706-0.178-0.1795\n",
      "[Test]: Precision: 0.0689-0.0607-0.048-0.0373 Recall: 0.1285-0.2058-0.3589-0.5008 NDCG: 0.1114-0.1383-0.1904-0.2352 MRR: 0.1814-0.1936-0.2005-0.2018\n",
      "Runing Epoch 075 train loss 3307.9828 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 076 train loss 3414.0247 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 077 train loss 3268.4750 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 078 train loss 3300.2970 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 079 train loss 3403.8453 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0658-0.0614-0.0539-0.0465 Recall: 0.0663-0.118-0.2266-0.3504 NDCG: 0.0824-0.0981-0.1358-0.1801 MRR: 0.1585-0.171-0.1785-0.18\n",
      "[Test]: Precision: 0.0693-0.0607-0.0482-0.0374 Recall: 0.1291-0.206-0.3602-0.5014 NDCG: 0.1124-0.139-0.1915-0.2361 MRR: 0.1831-0.1951-0.2021-0.2033\n",
      "Runing Epoch 080 train loss 3255.8329 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 081 train loss 3318.0194 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 082 train loss 3352.1219 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 083 train loss 3232.5846 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 084 train loss 3321.1421 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0658-0.061-0.0535-0.0462 Recall: 0.0658-0.1164-0.2234-0.3484 NDCG: 0.0823-0.0974-0.1347-0.1793 MRR: 0.1587-0.1711-0.1786-0.1801\n",
      "[Test]: Precision: 0.0693-0.0607-0.0482-0.0372 Recall: 0.1298-0.2062-0.359-0.4984 NDCG: 0.1127-0.1392-0.1913-0.2353 MRR: 0.1843-0.1961-0.2029-0.2042\n",
      "Runing Epoch 085 train loss 3286.2172 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 086 train loss 3279.6510 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 087 train loss 3338.1180 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 088 train loss 3242.0828 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 089 train loss 3317.0170 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0652-0.0607-0.0534-0.046 Recall: 0.0657-0.1164-0.2242-0.3475 NDCG: 0.0817-0.0971-0.1346-0.1788 MRR: 0.1578-0.1703-0.1777-0.1793\n",
      "[Test]: Precision: 0.0693-0.061-0.048-0.0371 Recall: 0.1296-0.2072-0.3591-0.4978 NDCG: 0.1121-0.1391-0.1908-0.2346 MRR: 0.1828-0.1947-0.2014-0.2027\n",
      "Runing Epoch 090 train loss 3196.8934 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 091 train loss 3156.1150 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 092 train loss 3203.2856 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 093 train loss 3227.0022 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 094 train loss 3238.3982 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "[Valid]: Precision: 0.0645-0.0604-0.0531-0.0459 Recall: 0.0646-0.1153-0.2223-0.3467 NDCG: 0.081-0.0965-0.1337-0.1781 MRR: 0.1583-0.1708-0.1783-0.1799\n",
      "[Test]: Precision: 0.0689-0.0605-0.0477-0.0369 Recall: 0.129-0.2059-0.3565-0.4946 NDCG: 0.1113-0.1381-0.1895-0.2332 MRR: 0.1815-0.1935-0.2001-0.2014\n",
      "Runing Epoch 095 train loss 3245.6909 costs 00: 00: 11\n",
      "------------------------------------------------------\n",
      "Runing Epoch 096 train loss 3127.4558 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 097 train loss 3207.1756 costs 00: 00: 04\n",
      "------------------------------------------------------\n",
      "Runing Epoch 098 train loss 3177.5504 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "Runing Epoch 099 train loss 3250.3680 costs 00: 00: 03\n",
      "------------------------------------------------------\n",
      "------------------\n",
      "======================================================\n",
      "End. Best Epoch 080 \n",
      "[Valid]: Precision: 0.0658-0.0614-0.0539-0.0465 Recall: 0.0663-0.118-0.2266-0.3504 NDCG: 0.0824-0.0981-0.1358-0.1801 MRR: 0.1585-0.171-0.1785-0.18\n",
      "[Test]: Precision: 0.0693-0.0607-0.0482-0.0374 Recall: 0.1291-0.206-0.3602-0.5014 NDCG: 0.1124-0.139-0.1915-0.2361 MRR: 0.1831-0.1951-0.2021-0.2033\n",
      "End time:  2023-11-11 17:21:02\n"
     ]
    }
   ],
   "source": [
    "best_recall, best_epoch = -100, 0\n",
    "best_test_result = None\n",
    "print(\"Start training...\")\n",
    "lr_adjust_times = 0\n",
    "all_lr = [args.lr*i for i in [1, 0.1, 0.01]]\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    if epoch - best_epoch >= 20:\n",
    "        print('-'*18)\n",
    "        break\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    batch_count = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_count += 1\n",
    "        optimizer.zero_grad()\n",
    "        losses = diffusion.training_losses(model, batch, args.sample_style, args.reweight)\n",
    "        loss = losses[\"loss\"].mean()\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        valid_results = evaluate(test_loader, valid_y_data, train_data, eval(args.topN))\n",
    "        if args.tst_w_val:\n",
    "            test_results = evaluate(test_twv_loader, test_y_data, mask_tv, eval(args.topN))\n",
    "        else:\n",
    "            test_results = evaluate(test_loader, test_y_data, mask_tv, eval(args.topN))\n",
    "        evaluate_utils.print_results(None, valid_results, test_results)\n",
    "\n",
    "        if valid_results[1][1] > best_recall: # recall@20 as selection\n",
    "            best_recall, best_epoch = valid_results[1][1], epoch\n",
    "            best_results = valid_results\n",
    "            best_test_results = test_results\n",
    "            \n",
    "    print(\"Runing Epoch {:03d} \".format(epoch) + 'train loss {:.4f}'.format(total_loss) + \" costs \" + time.strftime(\n",
    "                        \"%H: %M: %S\", time.gmtime(time.time()-start_time)))\n",
    "    print('---'*18)\n",
    "\n",
    "print('==='*18)\n",
    "print(\"End. Best Epoch {:03d} \".format(best_epoch))\n",
    "evaluate_utils.print_results(None, best_results, best_test_results)   \n",
    "print(\"End time: \", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecfaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
